# üß† Vulngard

Ce projet repose sur **Ollama** et le mod√®le **deepseek-coder:6.7b** pour la d√©tection de code vuln√©rable dans des projets OpenSource en se basant uniquement sur la description des CVE.

---

## üöÄ Installation rapide

### 1. Installer Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

V√©rifiez l'installation :

```bash
ollama --version
```

---

### 2. T√©l√©charger le mod√®le `deepseek-coder:6.7b`

```bash
ollama pull deepseek-coder:6.7b
```

---

### 3. Cloner ce d√©p√¥t et cr√©er le mod√®le personnalis√©

```bash
git clone git@github.com:nwOLXTYv/VulnGuard.git
cd VulnGuard
```

> Le fichier `Modelfile` est d√©j√† pr√©sent dans ce d√©p√¥t.

Cr√©ez votre mod√®le :

```bash
ollama create Michel -f ./Modelfile
```

---

### 4. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

---

## üõ†Ô∏è Utilisation du script VulnGuard

VulnGuard est un outil d'analyse de vuln√©rabilit√©s qui utilise les mod√®les Ollama pour √©valuer les vuln√©rabilit√©s de code. Voici comment l'utiliser :

### Ex√©cution du script

```bash
python3 vulnguard.py
```

### Fonctionnement pas √† pas

1. **Select an LLM Model:**

- VulnGuard will list the available Ollama models.
- Choose a model by entering the corresponding number or name. If you press Enter without making a selection, VulnGuard will use the default model (Michel:latest if available).

2. **Analyze Vulnerabilities:**

- Enter the CVE ID you want to analyze when prompted.
- Provide the path to the diff file location that corresponds to the CVE.
- VulnGuard will process the diff file and analyze it using the selected LLM model.

3. **Review Results:**

- After the analysis, VulnGuard will output the results, indicating whether the code is vulnerable or safe.
- You can choose to analyze another vulnerability or exit the tool.

---

## üß† Pourquoi `deepseek-coder:6.7b` ?

Nous avons test√© plusieurs mod√®les avec diff√©rentes tailles (nombre de n≈ìuds) pour trouver le **meilleur compromis entre qualit√© et rapidit√©**.

- `deepseek-coder:6.7b` a montr√© d'excellents r√©sultats
- Fonctionne bien sur une carte graphique 16 Go (ex: AMD RX 7800XT)

üìå Si vous souhaitez explorer d'autres mod√®les, visitez :

üëâ [https://ollama.com/search](https://ollama.com/search)

Modifiez simplement la ligne `FROM` du `Modelfile`, par exemple :

```Dockerfile
FROM mistral:7b
```

> üí° Le suffixe `:6.7b`, `:34b`, etc. correspond au **nombre de n≈ìuds**.  
> Plus il est √©lev√©, meilleures sont les performances... mais plus la g√©n√©ration est lente.

---

## üì¥ Fonctionnement hors ligne

Une fois le mod√®le t√©l√©charg√©, **aucune connexion internet n'est requise**. Le LLM tourne enti√®rement en **local**, sans cloud.

---

## üîß Configuration avanc√©e

### Template de prompt personnalis√©

Par d√©faut, le script utilise un template situ√© dans `./docs/user-prompt.txt`. Vous pouvez cr√©er votre propre template avec les variables suivantes :
- `{{CVE_DESCRIPTION}}` : Description de la CVE
- `{{FILE_LOCATION}}` : Emplacement du fichier
- `{{DIFF_HUNK}}` : Code diff √† analyser

---

## üìã Commandes utiles

| Description                      | Commande                                         |
|----------------------------------|--------------------------------------------------|
| Installer Ollama                 | `curl -fsSL https://ollama.com/install.sh \| sh` |
| T√©l√©charger un mod√®le            | `ollama pull <model_name>`                       |
| Quitter le prompt                | `/bye`                                           |
| Cr√©er un mod√®le personnalis√©     | `ollama create Michel -f ./Modelfile`            |
| Lancer le mod√®le personnalis√©    | `ollama run Michel`                              |
| Voir les mod√®les install√©s       | `ollama list`                                    |
| Lancer VulnGuard                 | `python3 vulnguard.py`                           |

---

## üßë‚Äçüíª Contributeurs

- TAHRI Bahaaeddine
- MOREL Mathilde
- GOSSET Arthur